import os
import time
import copy
import datetime
import numpy as np
import streamlit as st

from PIL import Image
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

from prompt_selector import PromptTemplate_ja


class Chatter(object):

    def __init__(self):

        # åˆæœŸç”»é¢ã®è¡¨ç¤º
        self.init_page()

        # session_stateã«ä½•ã‚‚ãªã„æ™‚(=åˆå›ã®ã¿)ã«å®Ÿè¡Œã•ã‚Œã‚‹é–¢æ•°
        if not st.session_state:
            self.init_only_once()

        # session_stateã«modelãŒãªã„æ™‚ã«å†å®Ÿè¡Œã•ã‚Œã‚‹é–¢æ•°
        if "model" not in st.session_state:
            self.select_model()

        # å±¥æ­´ã®ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³ã®è¡¨ç¤º
        self.set_clear_chat_button()

        # é¸æŠã—ãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã®è¡¨ç¤º
        self.display_verbose()

        # å¯¾è©±éƒ¨
        self.run()

        return

    def init_only_once(self) -> None:
        out_dir = "./out"
        chatlog_dir = "./chatlog"
        os.makedirs(chatlog_dir, exist_ok=True)

        # å­¦ç¿’ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å¤‰æ•°ã«è¿½åŠ ã—ã¦ä¸‹ã•ã„
        task_names = (
            "jdd_multi_turn",
            "jwc_multi_turn",
            )

        # å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ•°ã«è¿½åŠ ã—ã¦ä¸‹ã•ã„
        model_cards = (
            "rinna/japanese-gpt-neox-3.6b-instruction-sft-v2",
            "rinna/youri-7b-chat",
            "cyberagent/calm2-7b-chat",
            "stabilityai/japanese-stablelm-instruct-beta-7b",
            "stockmark/stockmark-13b-instruct",
            "pfnet/plamo-13b-instruct"
            )

        user_icon = np.array(Image.open("./datasets/icons/humation/user1.png"))
        ai_icon = np.array(Image.open("./datasets/icons/humation/ai1.png"))

        st.session_state.out_dir = out_dir
        st.session_state.model_cards = model_cards
        st.session_state.task_names = task_names
        st.session_state.chatlog_dir = chatlog_dir
        st.session_state.user_icon = user_icon
        st.session_state.ai_icon = ai_icon
        st.session_state.chat_log = ["input,output,generation_time"]
        st.session_state.date = datetime.datetime.now().strftime('%Y_%m_%d_ %H_%M_%S')
        st.session_state.like_or_not = list()
        st.session_state.eos_error = """<eos>ãƒˆãƒ¼ã‚¯ãƒ³ãŒå‡ºåŠ›ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚å­¦ç¿’æ™‚ã®æ–‡æœ«ã«<eos>ãƒˆãƒ¼ã‚¯ãƒ³ãŒä»˜ä¸ã•ã‚Œã¦ã„ãªã„ã‹ã€å­¦ç¿’ãŒä¸ååˆ†ãªã“ã¨ãŒåŸå› ã¨ã—ã¦è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚å­¦ç¿’ãŒä¸ååˆ†ã¨ã¯ã€epochæ•°ãŒå°‘ãªã„ã‹ã€å­¦ç¿’ç‡ãŒæ¥µç«¯ã«å¤§ãã„ã‚‚ã—ãã¯å°ã•ã™ãã‚‹ã€å­¦ç¿’ç‡ã®schedulerãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ã€ç­‰ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚ã¾ãŸã€å­¦ç¿’æ™‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«'å‡ºåŠ›å¾Œã«<eos>ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å¿…ãšå‡ºåŠ›ã—ãªã•ã„ã€‚'ã¨ã„ã†æ–‡ç« ã‚’è¿½åŠ ã—ã¦å†å­¦ç¿’ã™ã‚‹ã¨æ”¹å–„ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚"""
        return

    def init_page(self) -> None:
        st.set_page_config(page_title="ğŸ’¬ æ—¥æœ¬èªå¯¾è©±ãƒ‡ãƒ¢")
        st.header("ğŸ–‹ï¸ 2. æ—¥æœ¬èªè‡ªç”±å¯¾è©±ãƒ¢ãƒ‡ãƒ«")
        st.sidebar.title("ğŸ“– 1. è¨€èªãƒ¢ãƒ‡ãƒ«ã®é¸æŠ")
        return

    def set_clear_chat_button(self) -> None:
        clear_button = st.sidebar.button("ä¼šè©±ã®ãƒªã‚»ãƒƒãƒˆ", key="chatlog_clear")

        if clear_button or "messages" not in st.session_state:
            st.session_state.messages = list()
        return

    def select_model(self):
        task_name = st.sidebar.selectbox(
            "å­¦ç¿’ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é¸æŠã—ã¦ä¸‹ã•ã„ã€‚",
            st.session_state.task_names,
            index=None,
            placeholder="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯...",
            )
        if not task_name:
            st.stop()

        model_name = st.sidebar.selectbox(
            "è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ä¸‹ã•ã„ã€‚",
            st.session_state.model_cards,
            index=None,
            placeholder="è¨€èªãƒ¢ãƒ‡ãƒ«ã¯...",
            )
        if not model_name:
            st.stop()

        with st.status("å¯¾è©±ãƒ¢ãƒ‡ãƒ«èµ·å‹•ä¸­...", expanded=True) as status:

            adapter_path = f"{st.session_state.out_dir}/{task_name}/{model_name}"

            st.write("1/4 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ä½œæˆ")
            prompter = PromptTemplate_ja(model_name)
            prompt_format = prompter.set_prompt_format()

            st.write("2/4 ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿")
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                )
            # tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # youri-7b-chat, stablelm-7bã«å¿…è¦

            st.write("3/4 å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map="auto",
                trust_remote_code=True,
                load_in_4bit=True,
                torch_dtype="auto",
                )
            # model.resize_token_embeddings(len(tokenizer)) # youri-7b-chat, stablelm-7bã«å¿…è¦

            st.write("4/4 Adapterãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿")
            model = PeftModel.from_pretrained(
                model,
                adapter_path,
                device_map="auto",
                )

            model.eval()

            print(type(model))
            status.update(
                label="å¯¾è©±ãƒ¢ãƒ‡ãƒ«èµ·å‹•å®Œäº†",
                state="complete",
                expanded=False
                )

        st.session_state.task_name = task_name
        st.session_state.model_name = model_name
        st.session_state.model = model
        st.session_state.tokenizer = tokenizer
        st.session_state.adapter_path = adapter_path
        st.session_state.prompt_format = prompt_format
        return

    def display_verbose(self):
        with st.sidebar:
            verbose = list()
            verbose.append(f"## å‡ºåŠ›å…ˆãƒ•ã‚©ãƒ«ãƒ€\n- {st.session_state.chatlog_dir}")
            verbose.append(f"## å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n- {st.session_state.task_name}")
            verbose.append(f"## å­¦ç¿’ãƒ¢ãƒ‡ãƒ«\n- {st.session_state.model_name}")
            verbose.append(f"## æ¨è«–æ‰‹æ³•\n- 4bité‡å­åŒ–")
            verbose = "\n".join(verbose)
            st.markdown(verbose)
        return

    def generate_prompt(self, messages: list, eos_token: str):
        pf = st.session_state.prompt_format
        prompt = list()

        for message in messages:
            role = pf["input"] if message["from"] == "user" else pf["output"]
            line = f"{pf['sep']}{role}{message['value']}"
            if role == pf["output"]:
                line += eos_token
            prompt.append(line)
        prompt.append(f"\n{pf['output']}")

        prompt = "\n".join(prompt)
        return prompt

    def generate_response(self, prompt):
        input_ids = st.session_state.tokenizer(
            prompt, 
            return_tensors="pt",
            truncation=True,
            add_special_tokens=False
            ).input_ids.cuda()

        outputs = st.session_state.model.generate(
            input_ids=input_ids,
            max_new_tokens=128,
            do_sample=True,
            temperature=0.7,
            top_p=0.75,
            top_k=40,
            no_repeat_ngram_size=2,
            )
        outputs = outputs[0].tolist()

        eos_indices = [i for i, x in enumerate(outputs) if x == st.session_state.tokenizer.eos_token_id]
        assert len(eos_indices) > 0, self.eos_error
        eos_index = eos_indices[-1]

        response = st.session_state.tokenizer.decode(outputs[:eos_index])

        return response

    def extract_answer(self, response):
        sentinel = st.session_state.prompt_format["output"]
        sentinelLoc = response.rfind(sentinel)

        if sentinelLoc >= 0:
            result = response[sentinelLoc+len(sentinel):].replace("\n", "")
        else:
            result = 'ã‚¨ãƒ©ãƒ¼ï¼šæŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒèª¤ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒé¸æŠã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ä¸‹ã•ã„ã€‚'
            with st.chat_message("assistant"):
                st.markdown(result)

        return result

    def reply(self) -> tuple:
        prompt = self.generate_prompt(
            st.session_state.messages,
            st.session_state.tokenizer.eos_token
            )
        answer = self.extract_answer(self.generate_response(prompt))
        return answer

    def _display_chatlog(self):
        messages = st.session_state.get("messages", [])
        for idx, message in enumerate(messages):

            if message["from"] == "gpt":
                with st.chat_message("assistant", avatar=st.session_state.ai_icon):
                    st.markdown(message["value"])
            else:
                with st.chat_message("user", avatar=st.session_state.user_icon):
                    st.markdown(message["value"])
        return

    def _dump_chatlog(self):
        model_name = copy.deepcopy(st.session_state.model_name)
        model_name = model_name.replace("/", "_")
        filename = f"{st.session_state.chatlog_dir}/{st.session_state.task_name}_{model_name}_{st.session_state.date}.csv"

        with open(filename, mode="w", encoding="utf-8") as o:
            o.write("\n".join(st.session_state.chat_log))
        return

    def run(self):

        if user_input := st.chat_input("æ–‡ç« ã‚’å…¥åŠ›ã—ã¾ã—ã‚‡ã†"):

            st.session_state.messages.append(
                {"from": "user", "value": user_input}
                )

            self._display_chatlog()

            with st.spinner("ç”Ÿæˆä¸­ ..."):
                begin = time.perf_counter()
                answer = self.reply()
                duration = time.perf_counter() - begin

            st.session_state.messages.append(
                {"from": "gpt", "value": answer}
                )

            with st.chat_message("assistant", avatar=st.session_state.ai_icon):
                st.markdown(answer)

            st.session_state.chat_log.append(f"{user_input},{answer},{duration:02f}")

        if st.sidebar.button("ä¼šè©±å±¥æ­´ã®ä¿å­˜", key="save"):
            self._dump_chatlog()
        return


def main():
    chatbot = Chatter()


if __name__ == "__main__":
    main()
